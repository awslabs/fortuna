{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb715506",
   "metadata": {},
   "source": [
    "# Sinusoidal regression\n",
    "In this notebook we show how to use Fortuna to obtain calibrated uncertainty estimates of predictions in a sinusoidal regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce608e0c",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, './../src')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46822e44",
   "metadata": {},
   "source": [
    "### Generate the data\n",
    "We first generate the data. These are couple of input and target variables following a sinusoidal relation perturbed by noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bca2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def make_sinusoidal(n_data: int, noise: float = 0.1, seed: int = 0):\n",
    "    np.random.seed(seed)\n",
    "    w = np.arange(30)[:, None] / 30\n",
    "    b = 2 * np.pi * np.arange(30)[:, None] / 30\n",
    "\n",
    "    x = np.random.normal(size=(n_data,))\n",
    "    y = np.cos(w * x + b).sum(0) + noise * np.random.normal(size=(n_data,))\n",
    "    return x[:, None], y[:, None]\n",
    "train_data = make_sinusoidal(n_data=10000, seed=0)\n",
    "val_data = make_sinusoidal(n_data=1000, seed=1)\n",
    "test_data = make_sinusoidal(n_data=1000, seed=2)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 1))\n",
    "axes[0].scatter(*train_data, s=1, label=\"training data\", c=\"C0\"); axes[0].legend()\n",
    "axes[1].scatter(*val_data, s=1, label=\"validation data\", c=\"C1\"); axes[1].legend()\n",
    "axes[2].scatter(*test_data, s=1, label=\"test data\", c=\"C2\"); axes[2].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fced4e2",
   "metadata": {},
   "source": [
    "### Convert data to a compatible data loader\n",
    "Fortuna helps you converting tuple of arrays into a compatible data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe830e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fortuna.data import DataLoader\n",
    "train_data_loader = DataLoader.from_array_data(train_data, batch_size=128, shuffle=True, prefetch=True)\n",
    "val_data_loader = DataLoader.from_array_data(val_data, batch_size=128, prefetch=True)\n",
    "test_data_loader = DataLoader.from_array_data(test_data, batch_size=128, prefetch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df51dd3",
   "metadata": {},
   "source": [
    "### Build a probabilistic regressor\n",
    "Let us build a probabilistic regressor. This is an interface object containing several attributes that you can configure, i.e. `model`, `likelihood_log_variance_model`, `prior`, `posterior_approximator`, `output_calibrator`. In this example, we use an MLP model for both mean and log-variance of the likelihood, a Deep Ensemble posterior approximator, and the default temperature scaling output calibrator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ad516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fortuna.prob_model import ProbRegressor\n",
    "from fortuna.model import MLP\n",
    "from fortuna.prob_model.posterior import DeepEnsemblePosteriorApproximator\n",
    "import flax.linen as nn\n",
    "output_dim = 1\n",
    "prob_model = ProbRegressor(\n",
    "    model=MLP(output_dim=output_dim, activations=(nn.tanh, nn.tanh)),\n",
    "    likelihood_log_variance_model=MLP(output_dim=output_dim),\n",
    "    posterior_approximator=DeepEnsemblePosteriorApproximator()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4fd8b0",
   "metadata": {},
   "source": [
    "### Train the probabilistic model: posterior fitting and calibration\n",
    "We can now train the probabilistic model. This includes fitting the posterior distribution and calibrating the probabilistic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dc0619",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fortuna.prob_model.fit_config import FitConfig, FitMonitor\n",
    "from fortuna.metric.regression import rmse\n",
    "status = prob_model.train(\n",
    "    train_data_loader=train_data_loader,\n",
    "    val_data_loader=val_data_loader,\n",
    "    calib_data_loader=val_data_loader,\n",
    "    fit_config=FitConfig(monitor=FitMonitor(metrics=(rmse,)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259b01bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,3))\n",
    "for i, s in enumerate(status[\"fit_status\"]):\n",
    "    plt.plot(s[\"loss\"], label=f\"run #{i+1}\")\n",
    "plt.legend()\n",
    "plt.title(\"loss decays\", fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b332ee38",
   "metadata": {},
   "source": [
    "### Estimate predictive statistics\n",
    "We can now compute some predictive statistics by invoking the `predictive` attribute of the probabilistic regressor, and the method of interest. Most predictive statistics, e.g. mean or variance, require a loader of input data points. You can easily get this from the data loader calling its method `to_inputs_loader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7881bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_log_probs = prob_model.predictive.log_prob(data_loader=test_data_loader)\n",
    "test_inputs_loader = test_data_loader.to_inputs_loader()\n",
    "test_means = prob_model.predictive.mean(inputs_loader=test_inputs_loader)\n",
    "test_aleatoric_variances = prob_model.predictive.aleatoric_variance(inputs_loader=test_inputs_loader)\n",
    "test_epistemic_variances = prob_model.predictive.epistemic_variance(inputs_loader=test_inputs_loader)\n",
    "test_variances = prob_model.predictive.variance(inputs_loader=test_inputs_loader, aleatoric_variances=test_aleatoric_variances, epistemic_variances=test_epistemic_variances)\n",
    "test_stds = prob_model.predictive.std(inputs_loader=test_inputs_loader, variances=test_variances)\n",
    "test_cred_intervals = prob_model.predictive.credible_interval(inputs_loader=test_inputs_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23792040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fortuna.data import InputsLoader\n",
    "mesh = np.linspace(-4, 4.2)\n",
    "mesh_loader = InputsLoader.from_array_inputs(mesh)\n",
    "mesh_mean = prob_model.predictive.mean(mesh_loader)\n",
    "mesh_std = prob_model.predictive.std(mesh_loader)\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.scatter(*test_data, s=1, color=\"C0\", label=\"test data\")\n",
    "plt.plot(mesh, mesh_mean, color=\"C1\", label=\"mean\")\n",
    "plt.fill_between(mesh,(mesh_mean - 2 * mesh_std).squeeze(1), (mesh_mean + 2 * mesh_std).squeeze(1), alpha=0.3, color=\"C0\", label=f\"+/- {2} std\")\n",
    "plt.legend(loc=\"lower right\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672a87d5",
   "metadata": {},
   "source": [
    "### Compute metrics\n",
    "Given the predictive statistics, we compute metrics to assess how well the probabilistic model fit the data, and how calibrated are uncertainty estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5005d83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fortuna.metric.regression import root_mean_squared_error, prediction_interval_coverage_probability\n",
    "test_targets = test_data_loader.to_array_targets()\n",
    "rmse = root_mean_squared_error(preds=test_means, targets=test_targets)\n",
    "cred_picp = prediction_interval_coverage_probability(lower_bounds=test_cred_intervals[:, 0], upper_bounds=test_cred_intervals[:, 1], targets=test_targets)\n",
    "print(f\"Test RMSE: {rmse}\")\n",
    "print(f\"PICP for 95% credible intervals of test inputs: {cred_picp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88ea484",
   "metadata": {},
   "source": [
    "### Conformal intervals\n",
    "The PICP metric shows that the 95% credible intervals above are not perfectly calibrated. Conformal prediction methods provide a way to correct them and improve their calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b423b925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fortuna.conformal.regression import QuantileConformalRegressor\n",
    "val_inputs_loader = val_data_loader.to_inputs_loader()\n",
    "val_cred_intervals = prob_model.predictive.credible_interval(inputs_loader=val_inputs_loader)\n",
    "test_conformal_intervals = QuantileConformalRegressor().conformal_interval(\n",
    "    val_lower_bounds=val_cred_intervals[:, 0], val_upper_bounds=val_cred_intervals[:, 1],\n",
    "    test_lower_bounds=test_cred_intervals[:, 0], test_upper_bounds=test_cred_intervals[:, 1],\n",
    "    val_targets=val_data_loader.to_array_targets(), error=0.05\n",
    ")\n",
    "conformal_picp = prediction_interval_coverage_probability(lower_bounds=test_conformal_intervals[:, 0], upper_bounds=test_conformal_intervals[:, 1], targets=test_targets)\n",
    "print(f\"PICP for 95% conformal intervals of test inputs: {conformal_picp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b757c8",
   "metadata": {},
   "source": [
    "Another possibility is to get conformal interval starting from a one-dimensinal uncertainty statistic, e.g. the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17babdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fortuna.conformal.regression import OneDimensionalUncertaintyConformalRegressor\n",
    "val_means = prob_model.predictive.mean(inputs_loader=val_inputs_loader)\n",
    "val_stds = prob_model.predictive.std(inputs_loader=val_inputs_loader)\n",
    "\n",
    "test_conformal_intervals2 = OneDimensionalUncertaintyConformalRegressor().conformal_interval(\n",
    "    val_preds=val_means, val_uncertainties=val_stds, test_preds=test_means, test_uncertainties=test_stds, \n",
    "    val_targets=val_data_loader.to_array_targets(), error=0.05\n",
    ")\n",
    "conformal_picp2 = prediction_interval_coverage_probability(lower_bounds=test_conformal_intervals2[:, 0], upper_bounds=test_conformal_intervals2[:, 1], targets=test_targets)\n",
    "print(f\"PICP for 95% conformal intervals of test inputs: {conformal_picp2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906bf1da",
   "metadata": {},
   "source": [
    "### What if we had had model outputs to start from?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cf8aab",
   "metadata": {},
   "source": [
    "If you have already trained a model and obtained model outputs, you can still use Fortuna to calibrate them, and estimate uncertainty. For educational purposes only, let us take the concatenation of predictive mean and log-variance as model outputs, and pretend these were generated with some other framework. Furthermore, we store arrays of validation and test target variables, and assume these were also given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f43dc4b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "val_outputs = np.concatenate((val_means, np.log(prob_model.predictive.variance(val_inputs_loader))), axis=-1)\n",
    "test_outputs = np.concatenate((test_means, np.log(test_variances)), axis=-1)\n",
    "\n",
    "val_targets = val_data_loader.to_array_targets()\n",
    "test_targets = test_data_loader.to_array_targets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86183d3d",
   "metadata": {},
   "source": [
    "We now invoke a calibration classifier, with default temperature scaling output calibrator, and calibrate the model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c2ec27",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from fortuna.calib_model.regression import CalibRegressor\n",
    "calib_model = CalibRegressor()\n",
    "calib_status = calib_model.calibrate(outputs=val_outputs, targets=val_targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3b2fa9",
   "metadata": {},
   "source": [
    "Similarly as above, we can now compute predictive statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7e60d8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_log_probs = calib_model.predictive.log_prob(outputs=test_outputs, targets=test_targets)\n",
    "test_means = calib_model.predictive.mean(outputs=test_outputs)\n",
    "test_variances = calib_model.predictive.variance(outputs=test_outputs)\n",
    "test_stds = calib_model.predictive.std(outputs=test_outputs, variances=test_variances)\n",
    "test_cred_intervals = calib_model.predictive.credible_interval(outputs=test_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e135f2",
   "metadata": {},
   "source": [
    "Then one can compute metrics and conformal intervals, exactly as done above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fortunaenv",
   "language": "python",
   "name": "fortunaenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}