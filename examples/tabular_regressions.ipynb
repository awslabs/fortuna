{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d5ffd4c",
   "metadata": {},
   "source": [
    "# Tabular regressions\n",
    "In this notebook we do regression and obtain conformal intervals of predictions for several tabular data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09de9a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q aws-fortuna pandas xlrd openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acac119",
   "metadata": {},
   "source": [
    "### Download regression datasets from UCI\n",
    "We create the infrastructure to download several regression data sets from [UCI](https://archive.ics.uci.edu/ml/datasets.php). Feel free to run this cell without looking into it, as it does not offer insights about Fortuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7aed0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import logging\n",
    "import os\n",
    "import zipfile\n",
    "from typing import Tuple\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from fortuna.data import DataLoader\n",
    "\n",
    "_ALL_REGRESSION_DATASETS = {}\n",
    "\n",
    "\n",
    "def add_regression(C):\n",
    "    _ALL_REGRESSION_DATASETS.update({C.name: C})\n",
    "    return C\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, name: str, url: str, directory: str):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.directory = directory\n",
    "\n",
    "    @property\n",
    "    def datadir(self):\n",
    "        return os.path.join(self.directory, self.name)\n",
    "\n",
    "    @property\n",
    "    def datapath(self):\n",
    "        return os.path.join(self.datadir, self.url.split(\"/\")[-1])\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def read(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        pass\n",
    "\n",
    "    def shuffle(self, X: np.array, Y: np.array, seed: int = 0):\n",
    "        N = X.shape[0]\n",
    "        perm = np.arange(N)\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(perm)\n",
    "        return X[perm], Y[perm]\n",
    "\n",
    "    def normalize(self, Z: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        Z_mean = np.mean(Z, 0, keepdims=True)\n",
    "        Z_std = 1e-6 + np.std(Z, 0, keepdims=True)\n",
    "        return (Z - Z_mean) / Z_std, Z_mean, Z_std\n",
    "\n",
    "    def preprocess(self, X: np.ndarray, Y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        X, self.X_mean, self.X_std = self.normalize(X)\n",
    "        Y, self.Y_mean, self.Y_std = self.normalize(Y)\n",
    "        return X, Y\n",
    "\n",
    "    def split(\n",
    "            self,\n",
    "            X: np.array,\n",
    "            Y: np.array,\n",
    "            prop_train: float = 0.8,\n",
    "            prop_val: float = 0.1,\n",
    "    ) -> Tuple[\n",
    "        Tuple[np.ndarray, np.ndarray],\n",
    "        Tuple[np.ndarray, np.ndarray],\n",
    "        Tuple[np.ndarray, np.ndarray],\n",
    "    ]:\n",
    "        N = X.shape[0]\n",
    "        n_train = int(N * prop_train)\n",
    "        n_val = int(N * prop_val)\n",
    "        train_data = X[:n_train], Y[:n_train]\n",
    "        val_data = X[n_train: n_train + n_val], Y[n_train: n_train + n_val]\n",
    "        test_data = X[n_train + n_val:], Y[n_train + n_val:]\n",
    "        return train_data, val_data, test_data\n",
    "\n",
    "    def batch(\n",
    "            self,\n",
    "            train_data: Tuple[np.ndarray, np.ndarray],\n",
    "            val_data: Tuple[np.ndarray, np.ndarray],\n",
    "            test_data: Tuple[np.ndarray, np.ndarray],\n",
    "            batch_size: int = 128,\n",
    "            shuffle_train: bool = False,\n",
    "            prefetch: bool = True,\n",
    "    ) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "        train_data_loader = DataLoader.from_array_data(\n",
    "            train_data, batch_size=batch_size, shuffle=shuffle_train, prefetch=prefetch\n",
    "        )\n",
    "        val_data_loader = DataLoader.from_array_data(\n",
    "            val_data, batch_size=batch_size, prefetch=prefetch\n",
    "        )\n",
    "        test_data_loader = DataLoader.from_array_data(\n",
    "            test_data, batch_size=batch_size, prefetch=prefetch\n",
    "        )\n",
    "        return train_data_loader, val_data_loader, test_data_loader\n",
    "\n",
    "    def load(\n",
    "            self,\n",
    "            prop_train: float = 0.8,\n",
    "            prop_val: float = 0.1,\n",
    "            batch_size: int = 128,\n",
    "            shuffle_train: bool = False,\n",
    "            prefetch: bool = True,\n",
    "    ) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "        X, Y = self.read()\n",
    "        X, Y = self.preprocess(X, Y)\n",
    "        train_data, val_data, test_data = self.split(\n",
    "            X, Y, prop_train=prop_train, prop_val=prop_val\n",
    "        )\n",
    "        return self.batch(\n",
    "            train_data,\n",
    "            val_data,\n",
    "            test_data,\n",
    "            batch_size=batch_size,\n",
    "            shuffle_train=shuffle_train,\n",
    "            prefetch=prefetch,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def needs_download(self):\n",
    "        return not os.path.isfile(self.datapath)\n",
    "\n",
    "    def download(self):\n",
    "        if self.needs_download:\n",
    "            logging.info(\"\\nDownloading {} data...\".format(self.name))\n",
    "\n",
    "            if not os.path.isdir(self.datadir):\n",
    "                os.mkdir(self.datadir)\n",
    "\n",
    "            filename = os.path.join(self.datadir, self.url.split(\"/\")[-1])\n",
    "            with urlopen(self.url) as response, open(filename, \"wb\") as out_file:\n",
    "                data = response.read()\n",
    "                out_file.write(data)\n",
    "\n",
    "            is_zipped = np.any([z in self.url for z in [\".gz\", \".zip\", \".tar\"]])\n",
    "            if is_zipped:\n",
    "                zip_ref = zipfile.ZipFile(filename, \"r\")\n",
    "                zip_ref.extractall(self.datadir)\n",
    "                zip_ref.close()\n",
    "\n",
    "            logging.info(\"Download completed.\".format(self.name))\n",
    "        else:\n",
    "            logging.info(\"{} dataset is already available.\".format(self.name))\n",
    "\n",
    "\n",
    "uci_base_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/\"\n",
    "\n",
    "\n",
    "@add_regression\n",
    "class Boston(Dataset):\n",
    "    name = \"boston\"\n",
    "    url = uci_base_url + \"housing/housing.data\"\n",
    "\n",
    "    def __init__(self, directory, name=name, url=url):\n",
    "        super().__init__(name=name, url=url, directory=directory)\n",
    "\n",
    "    def read(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        data = pd.read_fwf(self.datapath, header=None).values\n",
    "        return data[:, :-1], data[:, -1].reshape(-1, 1)\n",
    "\n",
    "\n",
    "@add_regression\n",
    "class Concrete(Dataset):\n",
    "    name = \"concrete\"\n",
    "    url = uci_base_url + \"concrete/compressive/Concrete_Data.xls\"\n",
    "\n",
    "    def __init__(self, directory, name=name, url=url):\n",
    "        super().__init__(name=name, url=url, directory=directory)\n",
    "\n",
    "    def read(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        data = pd.read_excel(self.datapath).values\n",
    "        return data[:, :-1], data[:, -1].reshape(-1, 1)\n",
    "\n",
    "\n",
    "@add_regression\n",
    "class Energy(Dataset):\n",
    "    name = \"energy\"\n",
    "    url = uci_base_url + \"00242/ENB2012_data.xlsx\"\n",
    "\n",
    "    def __init__(self, directory, name=name, url=url):\n",
    "        super().__init__(name=name, url=url, directory=directory)\n",
    "\n",
    "    def read(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        data = pd.read_excel(self.datapath).values[:, :-1]\n",
    "        return data[:, :-1], data[:, -1].reshape(-1, 1)\n",
    "\n",
    "\n",
    "@add_regression\n",
    "class Naval(Dataset):\n",
    "    name = \"naval\"\n",
    "    url = uci_base_url + \"00316/UCI%20CBM%20Dataset.zip\"\n",
    "\n",
    "    def __init__(self, directory, name=name, url=url):\n",
    "        super().__init__(name=name, url=url, directory=directory)\n",
    "\n",
    "    @property\n",
    "    def datapath(self):\n",
    "        return os.path.join(self.datadir, \"UCI CBM Dataset/data.txt\")\n",
    "\n",
    "    def read(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        data = pd.read_fwf(self.datapath, header=None).values\n",
    "        X = data[:, :-2]\n",
    "        Y = data[:, -2].reshape(-1, 1)\n",
    "        X = np.delete(X, [8, 11], axis=1)\n",
    "        return X, Y\n",
    "\n",
    "\n",
    "@add_regression\n",
    "class Power(Dataset):\n",
    "    name = \"power\"\n",
    "    url = uci_base_url + \"00294/CCPP.zip\"\n",
    "\n",
    "    def __init__(self, directory, name=name, url=url):\n",
    "        super().__init__(name=name, url=url, directory=directory)\n",
    "\n",
    "    @property\n",
    "    def datapath(self):\n",
    "        return os.path.join(self.datadir, \"CCPP/Folds5x2_pp.xlsx\")\n",
    "\n",
    "    def read(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        data = pd.read_excel(self.datapath).values\n",
    "        return data[:, :-1], data[:, -1].reshape(-1, 1)\n",
    "\n",
    "\n",
    "@add_regression\n",
    "class Protein(Dataset):\n",
    "    name = \"protein\"\n",
    "    url = uci_base_url + \"00265/CASP.csv\"\n",
    "\n",
    "    def __init__(self, directory, name=name, url=url):\n",
    "        super().__init__(name=name, url=url, directory=directory)\n",
    "\n",
    "    def read(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        data = pd.read_csv(self.datapath).values\n",
    "        return data[:, 1:], data[:, 0].reshape(-1, 1)\n",
    "\n",
    "\n",
    "@add_regression\n",
    "class WineRed(Dataset):\n",
    "    name = \"winered\"\n",
    "    url = uci_base_url + \"wine-quality/winequality-red.csv\"\n",
    "\n",
    "    def __init__(self, directory, name=name, url=url):\n",
    "        super().__init__(name=name, url=url, directory=directory)\n",
    "\n",
    "    def read(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        data = pd.read_csv(self.datapath, delimiter=\";\").values\n",
    "        return data[:, :-1], data[:, -1].reshape(-1, 1)\n",
    "\n",
    "\n",
    "@add_regression\n",
    "class WineWhite(Dataset):\n",
    "    name = \"winewhite\"\n",
    "    url = uci_base_url + \"wine-quality/winequality-white.csv\"\n",
    "\n",
    "    def __init__(self, directory, name=name, url=url):\n",
    "        super().__init__(name=name, url=url, directory=directory)\n",
    "\n",
    "    def read(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        data = pd.read_csv(self.datapath, delimiter=\";\").values\n",
    "        return data[:, :-1], data[:, -1].reshape(-1, 1)\n",
    "\n",
    "\n",
    "@add_regression\n",
    "class Yacht(Dataset):\n",
    "    name = \"yacht\"\n",
    "    url = uci_base_url + \"/00243/yacht_hydrodynamics.data\"\n",
    "\n",
    "    def __init__(self, directory, name=name, url=url):\n",
    "        super().__init__(name=name, url=url, directory=directory)\n",
    "\n",
    "    def read(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        data = pd.read_fwf(self.datapath, header=None).values[:-1, :]\n",
    "        return data[:, :-1], data[:, -1].reshape(-1, 1)\n",
    "\n",
    "\n",
    "regression_datasets = list(_ALL_REGRESSION_DATASETS.keys())\n",
    "regression_datasets.sort()\n",
    "\n",
    "\n",
    "def download_regression_dataset(name, directory, *args, **kwargs):\n",
    "    dataset = _ALL_REGRESSION_DATASETS[name](directory, *args, **kwargs)\n",
    "    dataset.download()\n",
    "\n",
    "\n",
    "def download_all_regression_datasets(directory, *args, **kwargs):\n",
    "    for name in list(_ALL_REGRESSION_DATASETS.keys()):\n",
    "        download_regression_dataset(name, directory, *args, **kwargs)\n",
    "        \n",
    "def load_regression_dataset(\n",
    "    name, dir, *args, **kwargs\n",
    ") -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    dataset = _ALL_REGRESSION_DATASETS[name](dir)\n",
    "    return dataset.load(*args, **kwargs)\n",
    "        \n",
    "download_all_regression_datasets(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b0a563",
   "metadata": {},
   "source": [
    "### Regression on each data set\n",
    "For each data set, we build a probabilistic regressor using MLP models for both mean and log-variance of the likelihood. We fit the posterior distribution with the default SWAG method, and calibrate the probabilistic model with a temperature scaling approach. Finally, compute 95% credible intervals and calibrate them with a quantile conformal method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1138a21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fortuna.prob_model import ProbRegressor\n",
    "from fortuna.prob_model.fit_config import FitConfig, FitMonitor, FitOptimizer\n",
    "from fortuna.model import MLP\n",
    "from fortuna.conformal.regression import QuantileConformalRegressor\n",
    "from fortuna.metric.regression import rmse, picp\n",
    "import optax\n",
    "import tempfile\n",
    "all_status = dict()\n",
    "all_metrics = dict()\n",
    "\n",
    "for dataset_name in regression_datasets:\n",
    "    with tempfile.TemporaryDirectory() as data_dir:\n",
    "        # download and load data\n",
    "        download_regression_dataset(dataset_name, data_dir)\n",
    "        train_data_loader, val_data_loader, test_data_loader = load_regression_dataset(dataset_name, data_dir, shuffle_train=True, batch_size=512)\n",
    "        \n",
    "        # find output dimension\n",
    "        for batch_inputs, batch_targets in train_data_loader:\n",
    "            output_dim = batch_targets.shape[-1]\n",
    "            break\n",
    "\n",
    "        # define probabilistic regressor\n",
    "        prob_model = ProbRegressor(\n",
    "            model=MLP(output_dim=output_dim),\n",
    "            likelihood_log_variance_model=MLP(output_dim=output_dim),\n",
    "        )\n",
    "        \n",
    "        # train the probabilistic regression\n",
    "        all_status[dataset_name] = prob_model.train(\n",
    "            train_data_loader=train_data_loader,\n",
    "            val_data_loader=val_data_loader,\n",
    "            calib_data_loader=val_data_loader,\n",
    "            map_fit_config=FitConfig(monitor=FitMonitor(metrics=(rmse,), early_stopping_patience=2), optimizer=FitOptimizer(method=optax.adam(1e-1), n_epochs=100)),\n",
    "            fit_config=FitConfig(monitor=FitMonitor(metrics=(rmse,)), optimizer=FitOptimizer(method=optax.adam(1e-1), n_epochs=100))\n",
    "        )\n",
    "        \n",
    "        # compute predictive statistics\n",
    "        test_inputs_loader = test_data_loader.to_inputs_loader()\n",
    "        test_means = prob_model.predictive.mean(inputs_loader=test_inputs_loader)\n",
    "        test_cred_intervals = prob_model.predictive.credible_interval(inputs_loader=test_inputs_loader)\n",
    "        \n",
    "        # calibrate the credibility intervals\n",
    "        val_inputs_loader = val_data_loader.to_inputs_loader()\n",
    "        val_cred_intervals = prob_model.predictive.credible_interval(inputs_loader=val_inputs_loader)\n",
    "        test_conformal_intervals = QuantileConformalRegressor().conformal_interval(\n",
    "            val_lower_bounds=val_cred_intervals[:, 0], val_upper_bounds=val_cred_intervals[:, 1],\n",
    "            test_lower_bounds=test_cred_intervals[:, 0], test_upper_bounds=test_cred_intervals[:, 1],\n",
    "            val_targets=val_data_loader.to_array_targets(), error=0.05\n",
    "        )\n",
    "        \n",
    "        # compute metrics\n",
    "        all_metrics[dataset_name] = dict()\n",
    "        all_metrics[dataset_name][\"picp\"] = picp(lower_bounds=test_conformal_intervals[:, 0], upper_bounds=test_conformal_intervals[:, 1], targets=test_data_loader.to_array_targets())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7361bb2",
   "metadata": {},
   "source": [
    "SWAG invokes a Maximum-A-Posteriori (MAP) estimation algorithm and starts from there to approximate the posterior distribution. The following cell shows the loss decay during MAP for each data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc65d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(1, len(all_status), figsize=(20, 2))\n",
    "for i, dataset_name in enumerate(all_status):\n",
    "    axes[i].set_title(dataset_name)\n",
    "    axes[0].set_ylabel(\"MAP loss decay\")\n",
    "    axes[i].plot(all_status[dataset_name][\"fit_status\"][\"map\"][\"loss\"])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c489300d",
   "metadata": {},
   "source": [
    "We finally plot the Prediction Interval Coverage Probability (PICP) for each data set. Since the credible intervals are computed with a 95% coverage, we should expect PICPs to be as close as possible to 0.95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a46ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 3))\n",
    "plt.grid()\n",
    "plt.axhline(0.95, color=\"red\", alpha=0.5, linestyle=\"--\")\n",
    "plt.bar(regression_datasets, [all_metrics[dataset_name][\"picp\"] for dataset_name in all_metrics], width=0.1)\n",
    "plt.ylabel(\"PICP\", fontsize=12)\n",
    "plt.yticks(list(plt.yticks()[0]) + [0.95])\n",
    "plt.ylim([0, 1.1])\n",
    "for dataset_name in regression_datasets:\n",
    "    _picp = all_metrics[dataset_name][\"picp\"]\n",
    "    plt.annotate(str(round(float(_picp), 2)), (dataset_name, _picp + 0.01), fontsize=12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uq",
   "language": "python",
   "name": "uq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
