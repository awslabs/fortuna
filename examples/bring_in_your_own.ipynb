{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b8ae4eb",
   "metadata": {},
   "source": [
    "# Bring in your own objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c813cbc6",
   "metadata": {},
   "source": [
    "When constructing a probabilistic model, you can bring your own model, prior distribution and output calibrator. Let's make some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48945cf1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cf40ee",
   "metadata": {},
   "source": [
    "## Bring in your own model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57044ad",
   "metadata": {},
   "source": [
    "As an example, we show how to construct an arbitrary Convolutional Neural Network (CNN) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd92fe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        x = x.reshape((x.shape[0], -1))  # flatten\n",
    "        x = nn.Dense(features=256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=10)(x)\n",
    "        x = nn.log_softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587b8ee1",
   "metadata": {},
   "source": [
    "We now set it as a model in a probabilistic classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90933169",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fortuna.prob_model.classification import ProbClassifier\n",
    "prob_model = ProbClassifier(model=CNN())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0e0ff0",
   "metadata": {},
   "source": [
    "Done. Let's check that it works by initializing its parameters and doing a forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788e6fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "x = jnp.zeros((1, 64, 64, 10))\n",
    "variables = prob_model.model.init(random.PRNGKey(0), x)\n",
    "prob_model.model.apply(variables, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847fc4e9",
   "metadata": {},
   "source": [
    "## Bring in your own prior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2873712a",
   "metadata": {},
   "source": [
    "As an example, we show how to construct a multi-dimensional uniform prior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dcb2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fortuna.prob_model.prior import Prior\n",
    "from fortuna.typing import Params\n",
    "from typing import Optional\n",
    "from fortuna.utils.random import generate_rng_like_tree\n",
    "from jax.flatten_util import ravel_pytree\n",
    "from jax.tree_util import tree_map\n",
    "from jax._src.prng import PRNGKeyArray\n",
    "import jax.numpy as jnp\n",
    "\n",
    "class Uniform(Prior):\n",
    "    def log_joint_prob(self, params: Params) -> float:\n",
    "        v = jnp.mean((ravel_pytree(params)[0] <= 1) & (ravel_pytree(params)[0] >= 0))\n",
    "        return jnp.where(v == 1., jnp.array(0), -jnp.inf)\n",
    "    \n",
    "    def sample(self, params_like: Params, rng: Optional[PRNGKeyArray] = None) -> Params:\n",
    "        if rng is None:\n",
    "            rng = self.rng.get()\n",
    "        keys = generate_rng_like_tree(rng, params_like)\n",
    "        return tree_map(lambda l, k: random.uniform(k, l.shape, l.dtype), params_like, keys,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b7576d",
   "metadata": {},
   "source": [
    "In the code below, we test the uniform prior we just created. In order to call `sample`, we will set `prior.rng` to a `RandomNumberGenerator` object, which automatically handles and updates random number generators starting from a random seed. This is usually automatically done by the probabilistic model, so you never need to worry about this. But in this case, since we are testing a derived class of `Prior` in isolation, we need this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9b1eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fortuna.utils.random import RandomNumberGenerator\n",
    "prior = Uniform()\n",
    "prior.rng = RandomNumberGenerator(seed=0)\n",
    "params_in = dict(a=jnp.array([1.]), b=jnp.array([[0.]]), c=jnp.array([0.5, 1.]))\n",
    "params_out = dict(a=jnp.array([1.]), b=jnp.array([[0.]]), c=jnp.array([3., 1.]))\n",
    "print(f\"log-prob(params_in): {prior.log_joint_prob(params_in)}\")\n",
    "print(f\"log-prob(params_out): {prior.log_joint_prob(params_out)}\")\n",
    "print(f\"sample: {prior.sample(params_in)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ec8313",
   "metadata": {},
   "source": [
    "To use your your uniform prior in Fortuna, just set it as the `prior` parameter of your `ProbClassifier` or `ProbRegressor`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cf55c7",
   "metadata": {},
   "source": [
    "## Bring in your own output calibrator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a761ed",
   "metadata": {},
   "source": [
    "As an example, we show how to construct an MLP output calibrator. Mind that an output calibrator is just any Flax model, and as such you could also use the MLP pre-built in Fortuna. However, here we implement one from scratch for educational purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a4d427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "from typing import Tuple\n",
    "\n",
    "class MLP(nn.Module):\n",
    "  features: Tuple[int]\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    for feat in self.features[:-1]:\n",
    "      x = nn.relu(nn.Dense(feat)(x))\n",
    "    x = nn.Dense(self.features[-1])(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635d7a24",
   "metadata": {},
   "source": [
    "You can now set your MLP as the output calibrator of a probabilistic model, or a calibration model. We do it here for a calibration regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec5b9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fortuna.calib_model.regression import CalibRegressor\n",
    "calib_model = CalibRegressor(output_calibrator=MLP(features=(4, 2, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378b954b",
   "metadata": {},
   "source": [
    "Done. Let's check that it works by initializing its parameters and doing a forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b009519",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "x = jnp.ones((1, 10))\n",
    "variables = calib_model.output_calibrator.init(random.PRNGKey(0), x)\n",
    "calib_model.output_calibrator.apply(variables, x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fortunaenv",
   "language": "python",
   "name": "fortunaenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}